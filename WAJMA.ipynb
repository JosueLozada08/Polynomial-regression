{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b239eb22",
   "metadata": {},
   "source": [
    "# Polynomial Regression\n",
    "\n",
    "## Integrantes\n",
    "\n",
    "- Willy Corrales\n",
    "- Alejandro Haro\n",
    "- Josue Lozada\n",
    "- Alex Caicedo\n",
    "- Mateo Pillajo\n",
    "\n",
    "**Iniciales del grupo: WAJMA**\n",
    "\n",
    "---\n",
    "\n",
    "### Comentario general\n",
    "En este trabajo se analizan diferentes técnicas de regresión para modelar un conjunto de datos mediante regresión cuadrática y redes neuronales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f95505a",
   "metadata": {},
   "source": [
    "## 1. Importación de Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3809ddd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c8e7cd",
   "metadata": {},
   "source": [
    "### Comentario\n",
    "Se importan las librerías necesarias para el procesamiento de datos, entrenamiento de modelos, métricas de evaluación y visualización."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f2192c",
   "metadata": {},
   "source": [
    "## 2. Carga del Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f7751a",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/Machine1314/natural_computing/main/data_regression.csv\"\n",
    "df = pd.read_csv(url)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9354d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, 0].values.reshape(-1,1)\n",
    "y = df.iloc[:, 1].values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce295f05",
   "metadata": {},
   "source": [
    "### Comentario\n",
    "Se carga el dataset desde GitHub y se separan las variables independiente (X) y dependiente (y)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c66d39",
   "metadata": {},
   "source": [
    "## 3. Escalamiento de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5286cb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "y_scaled = scaler_y.fit_transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38f20ca",
   "metadata": {},
   "source": [
    "### Comentario\n",
    "El escalamiento es fundamental para que los algoritmos converjan correctamente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f677e17f",
   "metadata": {},
   "source": [
    "## 4. Regresión Cuadrática Analítica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358fe4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(degree=2)\n",
    "X_poly = poly.fit_transform(X_scaled)\n",
    "\n",
    "model_analytical = LinearRegression()\n",
    "model_analytical.fit(X_poly, y_scaled)\n",
    "\n",
    "y_pred_analytical = model_analytical.predict(X_poly)\n",
    "\n",
    "mse_analytical = mean_squared_error(y_scaled, y_pred_analytical)\n",
    "r2_analytical = r2_score(y_scaled, y_pred_analytical)\n",
    "\n",
    "mse_analytical, r2_analytical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e434c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(X_scaled, y_scaled)\n",
    "plt.plot(X_scaled, y_pred_analytical)\n",
    "plt.title(\"Regresión Cuadrática Analítica\")\n",
    "plt.xlabel(\"X Escalada\")\n",
    "plt.ylabel(\"Y Escalada\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcb6211",
   "metadata": {},
   "source": [
    "### Comentario\n",
    "Este modelo utiliza una solución matemática directa para encontrar la mejor curva cuadrática."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62ad2e4",
   "metadata": {},
   "source": [
    "## 5. Regresión por Descenso del Gradiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dc481e",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = len(X_scaled)\n",
    "theta0 = 0\n",
    "theta1 = 0\n",
    "theta2 = 0\n",
    "alpha = 0.01\n",
    "epochs = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a9d9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x):\n",
    "    return theta0 + theta1*x + theta2*(x**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc34d416",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history = []\n",
    "for _ in range(epochs):\n",
    "    y_hat = predict(X_scaled)\n",
    "\n",
    "    d0 = (-2/m)*np.sum(y_scaled - y_hat)\n",
    "    d1 = (-2/m)*np.sum((y_scaled - y_hat)*X_scaled)\n",
    "    d2 = (-2/m)*np.sum((y_scaled - y_hat)*(X_scaled**2))\n",
    "\n",
    "    theta0 -= alpha * d0\n",
    "    theta1 -= alpha * d1\n",
    "    theta2 -= alpha * d2\n",
    "\n",
    "    loss = np.mean((y_scaled - y_hat)**2)\n",
    "    loss_history.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77abc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_gd = predict(X_scaled)\n",
    "\n",
    "mse_gd = mean_squared_error(y_scaled, y_pred_gd)\n",
    "r2_gd = r2_score(y_scaled, y_pred_gd)\n",
    "\n",
    "mse_gd, r2_gd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242a28e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(X_scaled, y_scaled)\n",
    "plt.plot(X_scaled, y_pred_gd)\n",
    "plt.title(\"Regresión Cuadrática - Gradiente\")\n",
    "plt.xlabel(\"X Escalada\")\n",
    "plt.ylabel(\"Y Escalada\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc4dd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(loss_history)\n",
    "plt.title(\"Evolución del Error en Gradiente Descendente\")\n",
    "plt.xlabel(\"Iteraciones\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fb5035",
   "metadata": {},
   "source": [
    "### Comentario\n",
    "Aquí se observa cómo el error disminuye progresivamente hasta estabilizarse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edad1790",
   "metadata": {},
   "source": [
    "## 6. Redes Neuronales (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba453b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_10 = MLPRegressor(hidden_layer_sizes=(10,), max_iter=5000)\n",
    "mlp_10.fit(X_scaled, y_scaled.ravel())\n",
    "y_pred_10 = mlp_10.predict(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51719ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_50 = MLPRegressor(hidden_layer_sizes=(50,), max_iter=5000)\n",
    "mlp_50.fit(X_scaled, y_scaled.ravel())\n",
    "y_pred_50 = mlp_50.predict(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b24977f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_1000 = MLPRegressor(hidden_layer_sizes=(1000,), max_iter=5000)\n",
    "mlp_1000.fit(X_scaled, y_scaled.ravel())\n",
    "y_pred_1000 = mlp_1000.predict(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea222301",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(X_scaled, y_scaled)\n",
    "plt.plot(X_scaled, y_pred_10)\n",
    "plt.title(\"MLP con 10 neuronas\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46411dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(X_scaled, y_scaled)\n",
    "plt.plot(X_scaled, y_pred_50)\n",
    "plt.title(\"MLP con 50 neuronas\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26a329f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(X_scaled, y_scaled)\n",
    "plt.plot(X_scaled, y_pred_1000)\n",
    "plt.title(\"MLP con 1000 neuronas\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e61e892",
   "metadata": {},
   "source": [
    "### Comentario\n",
    "A mayor cantidad de neuronas aumenta la capacidad del modelo, pero también el riesgo de sobreajuste."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbb110d",
   "metadata": {},
   "source": [
    "## 7. Interpolación y Extrapolación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5547cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.linspace(X_scaled.min(), X_scaled.max()+2, 200).reshape(-1,1)\n",
    "mlp_curve = mlp_50.predict(x_test)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x_test, mlp_curve)\n",
    "plt.scatter(X_scaled, y_scaled)\n",
    "plt.title(\"Interpolación y Extrapolación con MLP\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55219eb9",
   "metadata": {},
   "source": [
    "### Comentario\n",
    "Se analiza cómo se comporta el modelo fuera del rango original de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df3b7a7",
   "metadata": {},
   "source": [
    "## 8. Tabla Comparativa Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f158bc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_10 = mean_squared_error(y_scaled, y_pred_10)\n",
    "r2_10 = r2_score(y_scaled, y_pred_10)\n",
    "\n",
    "mse_50 = mean_squared_error(y_scaled, y_pred_50)\n",
    "r2_50 = r2_score(y_scaled, y_pred_50)\n",
    "\n",
    "mse_1000 = mean_squared_error(y_scaled, y_pred_1000)\n",
    "r2_1000 = r2_score(y_scaled, y_pred_1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced63029",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({\n",
    "    \"Modelo\": [\"Analítico\", \"Gradiente\", \"MLP 10\", \"MLP 50\", \"MLP 1000\"],\n",
    "    \"MSE\": [mse_analytical, mse_gd, mse_10, mse_50, mse_1000],\n",
    "    \"R2\": [r2_analytical, r2_gd, r2_10, r2_50, r2_1000]\n",
    "})\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2672f66d",
   "metadata": {},
   "source": [
    "## 9. Conclusiones\n",
    "\n",
    "- El modelo analítico ofrece una solución estable.\n",
    "- El descenso del gradiente aproxima correctamente el comportamiento del sistema.\n",
    "- Las redes neuronales muestran mayor flexibilidad.\n",
    "- Un exceso de neuronas genera sobreajuste.\n",
    "\n",
    "### Comentario Final\n",
    "Este análisis permite comparar distintos enfoques de regresión y comprender sus ventajas y limitaciones."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
